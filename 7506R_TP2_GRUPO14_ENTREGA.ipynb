{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "______________________________________\n",
    "# <center>**Trabajo Practico Nº2 para la Materia *Organización de Datos***</center>\n",
    "\n",
    "*Integrantes*:\n",
    "- 103963\tCarolina Di Matteo\tcdimatteo@fi.uba.ar\n",
    "- 101231\tPablo Salvador Dimartino\tpdimartino@fi.uba.ar\n",
    "- 100113\tJuan Sebastian Burgos\tjsburgos@fi.uba.ar\n",
    "- 104415\tValentina Laura Correa\tvcorrea@fi.uba.ar\n",
    "\n",
    "*Grupo*: 14\n",
    "\n",
    "*Repositorio*: [github](https://github.com/valencorrea/7506R-2C2022-GRUPO14)\n",
    "\n",
    "*Curso*: Rodriguez\n",
    "\n",
    "*Cuatrimestre*: 2c2022\n",
    "\n",
    "Datos provistos por [properati](https://www.properati.com.ar).\n",
    "______________________________________\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ALhQpMoS2Do2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introducción"
   ],
   "metadata": {
    "id": "jrObdaVTw13E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El presente trabajo práctico es una continuación del ‘TP1: Propiedades en Venta’. \n",
    "\n",
    "En la entrega anterior se propuso aplicar técnicas de análisis exploratorio, preprocesamiento de datos, agrupamiento, clasificación y regresión. Siguiendo esta línea, y con el objetivo de continuar resolviendo problemas reales de ciencia de datos, en esta segunda parte se implementarán nuevos modelos predictivos a partir de los anteriormente mencionados. \n",
    "\n",
    "En esta oportunidad se buscará demostrar los conocimientos adquiridos sobre procesamiento del lenguaje natural, redes neuronales y ensamble de modelos. Para esto se utilizarán tanto datasets provistos por la materia, tomados de la página de la empresa Properati, como los generados por el grupo en el trabajo anterior."
   ],
   "metadata": {
    "id": "8Tot765gvLP2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Setup"
   ],
   "metadata": {
    "id": "civIgEQKvQ6A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importación de bibliotecas"
   ],
   "metadata": {
    "id": "vrqonJJjvVay"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: visualkeras in /home/juanse/.local/lib/python3.10/site-packages (0.0.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from visualkeras) (9.0.1)\r\n",
      "Requirement already satisfied: aggdraw>=1.3.11 in /home/juanse/.local/lib/python3.10/site-packages (from visualkeras) (1.3.15)\r\n",
      "Requirement already satisfied: numpy>=1.18.1 in /home/juanse/.local/lib/python3.10/site-packages (from visualkeras) (1.23.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install visualkeras"
   ],
   "metadata": {
    "id": "h6RrCiyq2Do7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3e7b3e93-b112-4351-a933-5d45f291d371"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pip install scikeras[tensorflow]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pip install scikeras[tensorflow-cpu]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pip install unidecode"
   ],
   "metadata": {
    "id": "vdZzPMc93I-C",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "045bdfc2-9805-43c2-c41d-1cd20aa7efce"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.6)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pip install types-all"
   ],
   "metadata": {
    "id": "7SrZ6Pvh8b_K",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2aff2586-af7e-4665-9009-c3ba548ce286"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: types-all in /home/juanse/.local/lib/python3.10/site-packages (1.0.0)\r\n",
      "Requirement already satisfied: types-click-spinner in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.1.13.1)\r\n",
      "Requirement already satisfied: types-six in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.16.21.4)\r\n",
      "Requirement already satisfied: types-tzlocal in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (4.2.2.2)\r\n",
      "Requirement already satisfied: types-docutils in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.19.1.1)\r\n",
      "Requirement already satisfied: types-dateparser in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.4.4)\r\n",
      "Requirement already satisfied: types-python-gflags in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.1.7.1)\r\n",
      "Requirement already satisfied: types-pyvmomi in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (8.0.0.0)\r\n",
      "Requirement already satisfied: types-PyMySQL in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.0.19.1)\r\n",
      "Requirement already satisfied: types-maxminddb in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.5.0)\r\n",
      "Requirement already satisfied: types-characteristic in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (14.3.7)\r\n",
      "Requirement already satisfied: types-cachetools in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (5.2.1)\r\n",
      "Requirement already satisfied: types-pysftp in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.2.17)\r\n",
      "Requirement already satisfied: types-DateTimeRange in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.2.8)\r\n",
      "Requirement already satisfied: types-ujson in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (5.6.0.0)\r\n",
      "Requirement already satisfied: types-MarkupSafe in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.10)\r\n",
      "Requirement already satisfied: types-Werkzeug in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.0.9)\r\n",
      "Requirement already satisfied: types-croniter in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.3.2.1)\r\n",
      "Requirement already satisfied: types-pycurl in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (7.45.2.0)\r\n",
      "Requirement already satisfied: types-pyaudio in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.2.16.4)\r\n",
      "Requirement already satisfied: types-aiofiles in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (22.1.0.4)\r\n",
      "Requirement already satisfied: types-toml in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.10.8.1)\r\n",
      "Requirement already satisfied: types-typed-ast in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.5.8.3)\r\n",
      "Requirement already satisfied: types-backports in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.1.3)\r\n",
      "Requirement already satisfied: types-certifi in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2021.10.8.3)\r\n",
      "Requirement already satisfied: types-python-dateutil in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.8.19.5)\r\n",
      "Requirement already satisfied: types-colorama in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.4.15.4)\r\n",
      "Requirement already satisfied: types-Pillow in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (9.3.0.4)\r\n",
      "Requirement already satisfied: types-atomicwrites in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.4.5.1)\r\n",
      "Requirement already satisfied: types-Markdown in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.4.2.1)\r\n",
      "Requirement already satisfied: types-enum34 in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.8)\r\n",
      "Requirement already satisfied: types-orjson in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.6.2)\r\n",
      "Requirement already satisfied: types-contextvars in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.4.7)\r\n",
      "Requirement already satisfied: types-Jinja2 in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.11.9)\r\n",
      "Requirement already satisfied: types-pyfarmhash in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.3.1)\r\n",
      "Requirement already satisfied: types-PyYAML in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (6.0.12.2)\r\n",
      "Requirement already satisfied: types-freezegun in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.10)\r\n",
      "Requirement already satisfied: types-click in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (7.1.8)\r\n",
      "Requirement already satisfied: types-JACK-Client in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.5.10.3)\r\n",
      "Requirement already satisfied: types-pymssql in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.1.0)\r\n",
      "Requirement already satisfied: types-dataclasses in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.6.6)\r\n",
      "Requirement already satisfied: types-fb303 in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.0.0)\r\n",
      "Requirement already satisfied: types-pkg-resources in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.1.3)\r\n",
      "Requirement already satisfied: types-futures in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.3.8)\r\n",
      "Requirement already satisfied: types-redis in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (4.3.21.6)\r\n",
      "Requirement already satisfied: types-decorator in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (5.1.8.1)\r\n",
      "Requirement already satisfied: types-mypy-extensions in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.4.24)\r\n",
      "Requirement already satisfied: types-Flask in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.6)\r\n",
      "Requirement already satisfied: types-protobuf in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (4.21.0.2)\r\n",
      "Requirement already satisfied: types-retry in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.9.9)\r\n",
      "Requirement already satisfied: types-scribe in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.0.0)\r\n",
      "Requirement already satisfied: types-PyJWT in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.7.1)\r\n",
      "Requirement already satisfied: types-frozendict in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.0.9)\r\n",
      "Requirement already satisfied: types-Routes in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.5.0)\r\n",
      "Requirement already satisfied: types-nmap in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.1.6)\r\n",
      "Requirement already satisfied: types-filelock in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.2.7)\r\n",
      "Requirement already satisfied: types-requests in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.28.11.6)\r\n",
      "Requirement already satisfied: types-boto in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.49.18.3)\r\n",
      "Requirement already satisfied: types-kazoo in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.1.3)\r\n",
      "Requirement already satisfied: types-termcolor in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.6)\r\n",
      "Requirement already satisfied: types-pytz in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2022.7.0.0)\r\n",
      "Requirement already satisfied: types-Deprecated in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.2.9)\r\n",
      "Requirement already satisfied: types-cryptography in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.3.23.2)\r\n",
      "Requirement already satisfied: types-waitress in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.1.4.3)\r\n",
      "Requirement already satisfied: types-docopt in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.6.11)\r\n",
      "Requirement already satisfied: types-emoji in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.1.0.1)\r\n",
      "Requirement already satisfied: types-backports-abc in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.5.2)\r\n",
      "Requirement already satisfied: types-singledispatch in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.7.5.1)\r\n",
      "Requirement already satisfied: types-tornado in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (5.1.1)\r\n",
      "Requirement already satisfied: types-ipaddress in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.0.8)\r\n",
      "Requirement already satisfied: types-mock in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (4.0.15.2)\r\n",
      "Requirement already satisfied: types-pathlib2 in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.3.0)\r\n",
      "Requirement already satisfied: types-python-slugify in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (7.0.0.1)\r\n",
      "Requirement already satisfied: types-paramiko in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.12.0.1)\r\n",
      "Requirement already satisfied: types-simplejson in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.18.0.0)\r\n",
      "Requirement already satisfied: types-first in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (2.0.5)\r\n",
      "Requirement already satisfied: types-chardet in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (5.0.4.1)\r\n",
      "Requirement already satisfied: types-annoy in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.17.8.1)\r\n",
      "Requirement already satisfied: types-polib in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.12.1)\r\n",
      "Requirement already satisfied: types-itsdangerous in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.6)\r\n",
      "Requirement already satisfied: types-openssl-python in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.1.3)\r\n",
      "Requirement already satisfied: types-xxhash in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.0.5.1)\r\n",
      "Requirement already satisfied: types-geoip2 in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (3.0.0)\r\n",
      "Requirement already satisfied: types-pyRFC3339 in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (1.1.1.1)\r\n",
      "Requirement already satisfied: types-tabulate in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (0.9.0.0)\r\n",
      "Requirement already satisfied: types-bleach in /home/juanse/.local/lib/python3.10/site-packages (from types-all) (5.0.3.1)\r\n",
      "Requirement already satisfied: types-urllib3<1.27 in /home/juanse/.local/lib/python3.10/site-packages (from types-requests->types-all) (1.26.25.4)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-26 00:25:25.416998: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Importación de librerías\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "import re\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from keras.metrics import MeanSquaredError\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score, precision_score, r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Configuración de Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "#Ejecución con Drive\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    IN_COLAB = True\n",
    "else:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB :\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    properati = pd.read_csv('/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/1d_df_reducido.csv')\n",
    "    properati_descrip = pd.read_csv('/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP1/properati_argentina_2021_decrip.csv')\n",
    "    stop_words = '/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/stopwords.txt'\n",
    "else:\n",
    "    # properati = pd.read_csv('./1d_df_reducido.csv')\n",
    "    df_train = pd.read_csv('./DATASETS/df_train_tp1.csv')\n",
    "    df_test = pd.read_csv('./DATASETS/df_test_tp1.csv')\n",
    "    properati_descrip = pd.read_csv('properati_argentina_2021_decrip.csv')\n",
    "    stop_words = 'stopwords'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funciones auxiliares"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def metricas_clasificacion(y_test, y_pred):\n",
    "    print(f'Accuracy: {round(accuracy_score(y_test, y_pred),2)}')\n",
    "    print(f'Precision: {round(precision_score(y_test, y_pred, average=\"macro\"),2)}')\n",
    "    print(f'Recall: {round(recall_score(y_test, y_pred, average=\"macro\"),2)}')\n",
    "    print(f'F1 Score: {round(f1_score(y_test, y_pred, average=\"macro\"),2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prediccion_y_metricas_regresion(regressor, x_test, y_test):\n",
    "\n",
    "  y_pred = regressor.predict(x_test)\n",
    "\n",
    "  print(f\"Se obtuvo un Score de {round(regressor.score(x_test, y_test)*100,3)}%\")\n",
    "\n",
    "  mse = metrics.mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = y_pred,\n",
    "        squared = True\n",
    "       )\n",
    "\n",
    "  print(f\"El error según la métrica 'Mean Square Error' de test es: {mse}\")\n",
    "\n",
    "  rmse = metrics.mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = y_pred,\n",
    "        squared = False\n",
    "       )\n",
    "\n",
    "  print(f\"El error según la métrica 'Root Mean Square Error' de test es: {rmse}\")\n",
    "\n",
    "  return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def format_aspects(line, word):\n",
    "    format = r\"\\W*([\\w]+)\"\n",
    "    n = 2\n",
    "    x = re.search(r'{}\\W*{}{}'.format(format*n, word, format*n), line)\n",
    "    if x is not None:\n",
    "        return x.group()\n",
    "    else:\n",
    "        return \"\""
   ],
   "metadata": {
    "id": "CCKsO-A5w7TQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_b_m_a(x):\n",
    "    mx = max(x[0], x[1], x[2])\n",
    "    if mx == x[0]:\n",
    "        return 'bajo'\n",
    "    elif mx == x[1]:\n",
    "        return 'medio'\n",
    "    elif mx == x[2]:\n",
    "        return 'alto'"
   ],
   "metadata": {
    "id": "3suAr0ciwY91",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def estandarizar(df, columns):\n",
    "  sscaler = StandardScaler()\n",
    "\n",
    "  for col in columns:\n",
    "    df[col] = sscaler.fit_transform(pd.DataFrame(df[col]))"
   ],
   "metadata": {
    "id": "0LF1eyB9l4nt",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def limpiar_values_de_aspects(df, aspects, values):\n",
    "    i = 0\n",
    "    for aspect in aspects:\n",
    "        for word in values[i]:\n",
    "            df[aspect] = df[aspect].apply(lambda line: word if word in line else line)\n",
    "        df[aspect] = df[aspect].apply(lambda line: line if len(line.split())<2 else '')\n",
    "        i = i+1"
   ],
   "metadata": {
    "id": "AAEy2PV92DpO",
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, x, y, splits, n):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=splits, n_repeats=n, random_state=1)\n",
    "    scores = cross_val_score(model, x, y, scoring='neg_mean_squared_error', cv=cv, verbose=1, n_jobs=3, error_score='raise')\n",
    "    return scores"
   ],
   "metadata": {
    "id": "tDt5-bPcEn_m",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_results(model_scores, name):\n",
    "    \n",
    "    model_names = list(model_scores.keys())\n",
    "    results = [model_scores[model] for model in model_names]\n",
    "    fig = go.Figure()\n",
    "    for model, result in zip(model_names, results):\n",
    "        fig.add_trace(go.Box(\n",
    "            y=result,\n",
    "            name=model,\n",
    "            boxpoints='all',\n",
    "            jitter=0.5,\n",
    "            whiskerwidth=0.2,\n",
    "            marker_size=2,\n",
    "            line_width=1)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "    title='Performance of Different Models Using 5-Fold Cross-Validation',\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Error Cuadrático Medio',\n",
    "    showlegend=False)\n",
    "    fig.show()"
   ],
   "metadata": {
    "id": "NImxsouY4HaJ",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparación de datasets"
   ],
   "metadata": {
    "id": "yG33nESmvkqN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train_x = df_train.drop([\"property_price\", \"tipo_precio_3\"], axis=\"columns\")\n",
    "df_train_y_regresion = df_train[\"property_price\"]\n",
    "df_train_y_clasificacion = df_train[\"tipo_precio_3\"]\n",
    "\n",
    "df_test_x = df_test.drop([\"property_price\", \"tipo_precio_3\"], axis=\"columns\")\n",
    "df_test_y_regresion = df_test[\"property_price\"]\n",
    "df_test_y_clasificacion = df_test[\"tipo_precio_3\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Procesamiento del Lenguaje Natural"
   ],
   "metadata": {
    "collapsed": false,
    "id": "DajLENWv2DpA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.a Ampliación del dataset\n",
    "___"
   ],
   "metadata": {
    "id": "mmjJ3qskuJyJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hacemos un merge del dataset original y el de descripciones, y quedémonos únicamente con las columnas `id` y `property_description`:"
   ],
   "metadata": {
    "id": "904YC-oLyddb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "df_descrip = pd.merge(df_train_x, properati_descrip, on=\"id\")\n",
    "df_descrip = df_descrip[[\"id\", \"property_description\"]]\n",
    "\n",
    "df_test_descrip = pd.merge(df_test_x, properati_descrip, on=\"id\")\n",
    "df_test_descrip = df_test_descrip[[\"id\", \"property_description\"]]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "vRbU2jpk2Do_",
    "outputId": "44fcabfb-0e8b-496a-881f-0de5fbc8f3e6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "                             id  \\\n0      ahcEMvB66wjPz0SYWZQDBw==   \n1      M0g0l0s6S13X+cZlGkUo8g==   \n2      V/KMMLRRx/Nn+g3m5lrW7A==   \n3      odR0QjYc3xtaYfqNJvbOSQ==   \n4      RqOcPIKYYZDG+CFMq2c1RA==   \n...                         ...   \n18600  g2fBcuyxiq3V0GdPATLFDw==   \n18601  2jfcV70r5M8iASKFbpFEqA==   \n18602  H4X7bNq04pK7U6fCcbxYNg==   \n18603  QWBP5Zp0TBUlFrzP9GO9bA==   \n18604  pKo6hHHPBZJydrHIn5S5Tg==   \n\n                                    property_description  \n0      Corredor Responsable: Juan Carlos Treco - CUCI...  \n1      Corredor Responsable: Micaela Perez / Lucas Fe...  \n2      Corredor Responsable: Gustavo Guastello - C.U....  \n3      PH A ESTRENAR SANCHEZ DE LORIA AL 1500 Y CONST...  \n4      EXCELENTE SEMIPISO 3 AMB C/BALCON LUMINOSO EN ...  \n...                                                  ...  \n18600  Venta DEPARTAMENTO 2 Ambientes Villa Ortuzar. ...  \n18601  Departamento a estrenar monoambiente, luminoso...  \n18602  Corredor Responsable: Daniel Acosta - CUCICBA ...  \n18603  El departamento se ubica en la PB y contiene:<...  \n18604  Departamento tipo PH en el hermoso edificio hi...  \n\n[18605 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>property_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ahcEMvB66wjPz0SYWZQDBw==</td>\n      <td>Corredor Responsable: Juan Carlos Treco - CUCI...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>M0g0l0s6S13X+cZlGkUo8g==</td>\n      <td>Corredor Responsable: Micaela Perez / Lucas Fe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>V/KMMLRRx/Nn+g3m5lrW7A==</td>\n      <td>Corredor Responsable: Gustavo Guastello - C.U....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>odR0QjYc3xtaYfqNJvbOSQ==</td>\n      <td>PH A ESTRENAR SANCHEZ DE LORIA AL 1500 Y CONST...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RqOcPIKYYZDG+CFMq2c1RA==</td>\n      <td>EXCELENTE SEMIPISO 3 AMB C/BALCON LUMINOSO EN ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18600</th>\n      <td>g2fBcuyxiq3V0GdPATLFDw==</td>\n      <td>Venta DEPARTAMENTO 2 Ambientes Villa Ortuzar. ...</td>\n    </tr>\n    <tr>\n      <th>18601</th>\n      <td>2jfcV70r5M8iASKFbpFEqA==</td>\n      <td>Departamento a estrenar monoambiente, luminoso...</td>\n    </tr>\n    <tr>\n      <th>18602</th>\n      <td>H4X7bNq04pK7U6fCcbxYNg==</td>\n      <td>Corredor Responsable: Daniel Acosta - CUCICBA ...</td>\n    </tr>\n    <tr>\n      <th>18603</th>\n      <td>QWBP5Zp0TBUlFrzP9GO9bA==</td>\n      <td>El departamento se ubica en la PB y contiene:&lt;...</td>\n    </tr>\n    <tr>\n      <th>18604</th>\n      <td>pKo6hHHPBZJydrHIn5S5Tg==</td>\n      <td>Departamento tipo PH en el hermoso edificio hi...</td>\n    </tr>\n  </tbody>\n</table>\n<p>18605 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_descrip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Busquemos aspectos de una propiedad utilizando la columna `property_description`."
   ],
   "metadata": {
    "id": "uCEL8YyUxYaa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cuántos registros nulos existen:"
   ],
   "metadata": {
    "id": "tAEE-G_QzCXj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 0 datos nulos.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hay {df_descrip['property_description'].isna().sum()} datos nulos.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydHTArN62DpA",
    "outputId": "532aec7a-b0bc-4f97-d56d-80ee53135a25"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cuáles son las 100 palabras más comunes en el campo de descripción de propiedades:"
   ],
   "metadata": {
    "id": "t4hiRECrz3Wa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "[('de', 857976),\n ('y', 507778),\n ('con', 401867),\n ('la', 290195),\n ('en', 278176),\n ('a', 243751),\n ('el', 169952),\n ('del', 153279),\n ('que', 140055),\n ('por', 133635),\n ('al', 125215),\n ('-', 113875),\n ('un', 103716),\n ('las', 90713),\n ('los', 88435),\n ('para', 88190),\n ('se', 63109),\n ('DE', 56457),\n ('son', 56420),\n ('2', 52790),\n ('es', 50384),\n ('una', 48927),\n ('3', 41258),\n ('cocina', 38889),\n ('ambientes', 38848),\n ('valor', 38420),\n ('esta', 38292),\n ('muy', 36893),\n ('x', 36800),\n ('Av.', 36630),\n ('comedor', 35535),\n ('baño', 35476),\n ('Y', 34141),\n ('CON', 33915),\n ('no', 33912),\n ('piso', 33348),\n ('o', 33050),\n ('/', 32253),\n ('tu', 32101),\n ('A', 32063),\n ('hasta', 30269),\n ('balcón', 29805),\n ('inmueble', 28725),\n ('casa', 28655),\n ('No', 28635),\n ('propiedad.', 28070),\n ('30%', 27684),\n ('departamento', 27670),\n ('EN', 27267),\n ('cuadras', 27069),\n ('Corredor', 26991),\n ('préstamo', 26920),\n ('cuota', 26780),\n ('medidas', 26422),\n ('Responsable:', 25970),\n ('living', 25952),\n ('4', 25848),\n ('Lendar', 25571),\n ('querés!', 25421),\n ('podés.', 25362),\n ('Accedé', 25354),\n ('Simulá', 25349),\n ('#', 25248),\n ('ID', 25231),\n ('MLS', 25227),\n ('CUCICBA', 24648),\n ('El', 24541),\n ('edificio', 24305),\n ('dos', 24304),\n ('cuenta', 24241),\n ('personas', 23936),\n ('completo', 23797),\n ('propiedad', 23309),\n ('salida', 23058),\n ('parte', 22277),\n (',', 22138),\n ('pisos', 21621),\n ('encuentra', 21617),\n ('frente', 21612),\n ('Las', 21073),\n ('1', 20964),\n ('amplio', 20885),\n ('Comprá', 20793),\n ('\\\\n\\\\n', 20732),\n ('vista', 19912),\n ('presente', 19511),\n ('dormitorio', 19499),\n ('Ley', 19317),\n ('gran', 19096),\n ('La', 18859),\n ('placard', 18483),\n ('espacio', 18475),\n ('metros', 18400),\n ('m2', 17976),\n ('Los', 17521),\n ('<br>', 17369),\n ('accesible', 17076),\n ('su', 16596),\n ('dormitorios', 16161),\n ('sobre', 16145)]"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(\" \".join(df_descrip[\"property_description\"]).split()).most_common(100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpAlq6jJ2DpC",
    "outputId": "ba6d7ea0-e0b6-4446-d3e2-550d723373d2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos observar que podríamos optimizar el texto mediante algunas técnicas de reducción y/o transformación. Entre otras:"
   ],
   "metadata": {
    "id": "iwnQ6tgt0MRV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminemos la etiqueta `<br>` de html:"
   ],
   "metadata": {
    "id": "qK78Ucya0Ugr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: line.replace(\"<br>\", \" \"))\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: line.replace(\"<br>\", \" \"))"
   ],
   "metadata": {
    "id": "_o_B51Zk0hGz"
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformemos todas las palabras a minúsculas, de modo que el contador no realice distinciones:"
   ],
   "metadata": {
    "id": "pyNp4Y520yHS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: line.lower())\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: line.lower())\n"
   ],
   "metadata": {
    "id": "VWBmcgfI04Ge"
   },
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quitemos los tíldes de las letras:"
   ],
   "metadata": {
    "id": "WpSPMtks1bZz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: unidecode.unidecode(line))\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: unidecode.unidecode(line))"
   ],
   "metadata": {
    "id": "KWHrTmAG1dGn"
   },
   "execution_count": 62,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [62], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_descrip[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_description\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_descrip[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_description\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m line: unidecode\u001B[38;5;241m.\u001B[39munidecode(line))\n\u001B[1;32m      2\u001B[0m df_test_descrip[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_description\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_test_descrip[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_description\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m line: unidecode\u001B[38;5;241m.\u001B[39munidecode(line))\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:4774\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[1;32m   4664\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4665\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4666\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4669\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4670\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4671\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4672\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4673\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4772\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4773\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4774\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1100\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1097\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# self.f is Callable\u001B[39;00m\n\u001B[0;32m-> 1100\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1151\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1150\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m-> 1151\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1152\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1153\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1154\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1155\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1158\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1159\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2919\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn [62], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(line)\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_descrip[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_description\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_descrip[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_description\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m line: \u001B[43munidecode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munidecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      2\u001B[0m df_test_descrip[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_description\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_test_descrip[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_description\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m line: unidecode\u001B[38;5;241m.\u001B[39munidecode(line))\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/unidecode/__init__.py:66\u001B[0m, in \u001B[0;36munidecode_expect_ascii\u001B[0;34m(string, errors, replace_str)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m string\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_unidecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstring\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplace_str\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/unidecode/__init__.py:121\u001B[0m, in \u001B[0;36m_unidecode\u001B[0;34m(string, errors, replace_str)\u001B[0m\n\u001B[1;32m    118\u001B[0m retval \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, char \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(string):\n\u001B[0;32m--> 121\u001B[0m     repl \u001B[38;5;241m=\u001B[39m \u001B[43m_get_repl_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchar\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m repl \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/unidecode/__init__.py:86\u001B[0m, in \u001B[0;36m_get_repl_str\u001B[0;34m(char)\u001B[0m\n\u001B[1;32m     82\u001B[0m codepoint \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mord\u001B[39m(char)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m codepoint \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0x80\u001B[39m:\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;66;03m# Already ASCII\u001B[39;00m\n\u001B[0;32m---> 86\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mchar\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m codepoint \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0xeffff\u001B[39m:\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;66;03m# No data on characters in Private Use Area and above.\u001B[39;00m\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminemos los símbolos:"
   ],
   "metadata": {
    "id": "tpUndCZy1j2X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: re.sub(r'[^\\w]', ' ', line))\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: re.sub(r'[^\\w]', ' ', line))"
   ],
   "metadata": {
    "id": "sWcF2SIJ1lfG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminemos los espacios múltiples entre palabras:"
   ],
   "metadata": {
    "id": "bdqDlvqN1rL1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(lambda line: re.sub(\"\\s\\s+\" , \" \", line))\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(lambda line: re.sub(\"\\s\\s+\" , \" \", line))"
   ],
   "metadata": {
    "id": "uQvajcwC2DpE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizando el contenido del archivo `stop_words.txt`, eliminemos palabras sin significado del datset y colocamos los cambios en uno nuevo:"
   ],
   "metadata": {
    "id": "BzcWZSJY4GPs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(stop_words) as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "f = lambda x: ' '.join([item for item in x.split() if item not in lines])\n",
    "\n",
    "df_descrip[\"property_description\"] = df_descrip[\"property_description\"].apply(f)\n",
    "df_test_descrip[\"property_description\"] = df_test_descrip[\"property_description\"].apply(f)"
   ],
   "metadata": {
    "id": "FrOeCCBY2DpF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego de estas transformaciones, veamos cuáles son las palabras más utilizadas:"
   ],
   "metadata": {
    "id": "JYXs-85EVQwx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"property_description\"]).split()).most_common(100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNYcv0DM2DpG",
    "outputId": "eed3ff96-3911-4d2a-a042-969e76b82e01"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seleccionemos los aspectos que nos parecen relevantes, para luego buscar sus posibles valores.\n",
    "\n",
    "Para esto, elegimos: `cocina`, `pisos`, `calefaccion`, `expensas`, `lavadero`, `balcon`, `cochera` y `aire` y limpiamos cualquier tipo de formato restante en el dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Ai_Odw_x2DpG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "aspectos = ['cocina', 'pisos', 'calefaccion', 'expensas', 'lavadero', 'balcon', 'cochera', 'aire']"
   ],
   "metadata": {
    "id": "BkHpD1gtB7e_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for word in aspectos:\n",
    "    df_descrip[word] = df_descrip[\"property_description\"].apply(lambda line: format_aspects(line, word))\n",
    "    df_test_descrip[word] = df_test_descrip[\"property_description\"].apply(lambda line: format_aspects(line, word))"
   ],
   "metadata": {
    "id": "AnaswxtR2DpH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cuáles son las 15 palabras más comunes para cada uno de los aspectos elegidos:"
   ],
   "metadata": {
    "id": "EDsk7Bjgx116"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cant_val_aspectos = 15"
   ],
   "metadata": {
    "id": "eYdLqsvGyNcj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `cocina`\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "wWJKdB85y1l0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"cocina\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "l-7v9FP82DpH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6bea24da-1ae1-4df4-eb9a-5fddb082398b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `cocina`, los valores podrían ser: \n",
    "- integrada\n",
    "- lavadero\n",
    "- completa"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6ACLlV0F2DpI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `pisos`"
   ],
   "metadata": {
    "id": "1XQ4TVyUzSqc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"pisos\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "UQQMsrnL2DpI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a261ec0d-9cd6-4277-98b0-7b18d0b28211"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `pisos`, los valores podrían ser: \n",
    "- porcelanato\n",
    "- parquet\n",
    "- madera"
   ],
   "metadata": {
    "collapsed": false,
    "id": "l39ZahiN2DpJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `calefaccion`"
   ],
   "metadata": {
    "id": "a4pG3YbW1wnt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"calefaccion\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "RPZApHfA2DpJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "67f4d899-74cf-4d93-ea99-c89946b6f262"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `calefaccion`, los valores podrían ser: \n",
    "- radiadores\n",
    "- radiante\n",
    "- central\n",
    "- individual"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Mt-7dfsl2DpK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `expensas`"
   ],
   "metadata": {
    "id": "XfzRZc861ysa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"expensas\"]).split()).most_common(cant_val_aspectos*2)"
   ],
   "metadata": {
    "id": "iRKPdGhA2DpK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e89ec902-f5d3-45e5-df24-45d796fd889b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `expensas`, los valores podrían ser: \n",
    "- servicios\n",
    "- impuestos \n",
    "- bajas"
   ],
   "metadata": {
    "collapsed": false,
    "id": "C7EiojY82DpK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `lavadero`"
   ],
   "metadata": {
    "id": "ZVVvuLxu11U0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"lavadero\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "sAzBKC5c2DpK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c811e281-914c-4804-fe87-064ada17a12d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `lavadero`, los valores podrían ser: \n",
    "- independiente\n",
    "- cocina\n",
    "- comedor"
   ],
   "metadata": {
    "collapsed": false,
    "id": "i8zxU-Co2DpL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `balcon`"
   ],
   "metadata": {
    "id": "0opxhwgx122i"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"balcon\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "geCmhGct2DpL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7f9103b9-03aa-4035-dd78-c3a1c435e833"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `balcon`, los valores podrían ser: \n",
    "- frente\n",
    "- amplio \n",
    "- terraza \n",
    "- salida \n",
    "- corrido\n",
    "- luminoso"
   ],
   "metadata": {
    "collapsed": false,
    "id": "GUiM00PQ2DpL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `cochera`"
   ],
   "metadata": {
    "id": "W05qxE7f138s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"cochera\"]).split()).most_common(cant_val_aspectos*2)"
   ],
   "metadata": {
    "id": "vF7x7sFV2DpL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aa1e9494-2db9-48dd-8101-5987d80ccb55"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `cochera`, los valores podrían ser: \n",
    "- fija\n",
    "- cubierta"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_sGYjhjF2DpM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Aspecto `aire`"
   ],
   "metadata": {
    "id": "ehAh9leY2Lfu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(\" \".join(df_descrip[\"aire\"]).split()).most_common(cant_val_aspectos)"
   ],
   "metadata": {
    "id": "eQlqeMRa2DpM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fa0a82b5-9f5b-4ab4-80b7-f0090e746727"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para `aire`, posibles valores son: \n",
    "- split \n",
    "- central \n",
    "- acondicionado"
   ],
   "metadata": {
    "collapsed": false,
    "id": "9hWHT90P2DpM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Consolidación de valores"
   ],
   "metadata": {
    "id": "fF4R61si2RJI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación creamos la variable `values`, que contiene los posibles valores para cada uno de los aspectos elegidos:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ZTu9pPYV2DpN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values_cocina = ['integrada' , 'lavadero' , 'completa']\n",
    "values_pisos = ['porcelanato' , 'parquet' , 'madera']\n",
    "values_calefaccion = ['radiadores' , 'radiante' , 'central' , 'individual']\n",
    "values_expensas = ['serviocios' , 'impuestos' , 'bajas']\n",
    "values_lavadero = ['independiente' , 'cocina' , 'comedor']\n",
    "values_balcon = ['frente' , 'amplio' , 'terraza' , 'salida' , 'corrido' , 'luminoso']\n",
    "values_cochera = ['fija' , 'cubierta']\n",
    "values_aire = ['split' , 'central' , 'acondicionado']"
   ],
   "metadata": {
    "id": "8woiCcO32DpN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "values = [values_cocina, values_pisos, values_calefaccion, values_expensas, values_lavadero, values_balcon, values_cochera, values_aire]"
   ],
   "metadata": {
    "id": "CFU_0j0EGUE5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "En primer lugar, creamos un dataset auxiliar que tenga los IDs y las columnas de los aspectos:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "B9JpAzOi2DpN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aux_df = df_descrip.copy()\n",
    "aux_df_test = df_test_descrip.copy()\n",
    "\n",
    "aux_df.drop('property_description', inplace=True, axis=1)\n",
    "aux_df_test.drop('property_description', inplace=True, axis=1)"
   ],
   "metadata": {
    "id": "Z617kPUE2DpN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego creamos una función a la que -pasándole un dataset, los aspectos y el listado de valores posibles- reemplace el contenido de las columnas por los valores correspondientes."
   ],
   "metadata": {
    "collapsed": false,
    "id": "qpHzefm72DpO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modificamos las columnas de los aspectos, para que sólo queden los valores correspondientes:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8A7lmncH2DpO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "limpiar_values_de_aspects(aux_df, aspectos, values)\n",
    "limpiar_values_de_aspects(aux_df_test, aspectos, values)\n",
    "aux_df.head(20)"
   ],
   "metadata": {
    "id": "WXhzGxCl2DpO",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "outputId": "3355d0c0-87e8-4707-b349-e4d9b468bfa9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por último hacemos el merge con el dataset original, teniendo en cuenta los IDs:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Ia2LxOLq2DpP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df_train_x.copy()\n",
    "df = pd.merge(df,aux_df, on=\"id\")\n",
    "df.drop(\"id\", inplace=True, axis=\"columns\")\n",
    "\n",
    "df_test = df_test_x.copy()\n",
    "df_test = pd.merge(df_test,aux_df_test, on=\"id\")\n",
    "df_test.drop(\"id\", inplace=True, axis=\"columns\")\n",
    "\n",
    "df.head(20)"
   ],
   "metadata": {
    "id": "I2x-X60t2DpP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "outputId": "9aa74e5a-0b67-4010-a31b-a2f67689e136"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exportación de Datos"
   ],
   "metadata": {
    "id": "rN3FutIEHrf2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exportamos los datasets generados:"
   ],
   "metadata": {
    "id": "xd-CoUsZHtL9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/DATASETS/1a_df_descrip.csv'\n",
    "else:\n",
    "  path = 'DATASETS/1a_df_descrip.csv'\n",
    "\n",
    "df_descrip.to_csv(path)"
   ],
   "metadata": {
    "id": "qr1jlB8QH16Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/DATASETS/1a_df_ampliado.csv'\n",
    "else:\n",
    "  path = 'DATASETS/1a_df_ampliado.csv'\n",
    "\n",
    "df.to_csv(path)"
   ],
   "metadata": {
    "id": "QTsLoqr-IHOX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.b Modelos\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fI1Fi95S2DpP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sin optimización de hiperparámetros"
   ],
   "metadata": {
    "id": "ldOrNGnEaE5H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenemos un modelo de XGBoost con los mismos hiperparámetros utilizados en el TP1."
   ],
   "metadata": {
    "id": "a2X0YkeUUL1X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Realizamos One Hot Encoding para las nuevas variables cualitativas:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "WqUMr9_r2DpQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['cocina', 'pisos', 'lavadero', 'calefaccion', 'expensas', 'balcon', 'cochera', 'aire'], drop_first=True)\n",
    "df_test_dummies = pd.get_dummies(df_test, columns=['cocina', 'pisos', 'lavadero', 'calefaccion', 'expensas', 'balcon', 'cochera', 'aire'], drop_first=True)\n",
    "df_dummies.head(5)"
   ],
   "metadata": {
    "id": "5e0r2Yj-2DpQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "outputId": "9345f93a-6a15-48ed-ebd9-8004cb87637f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dummies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos el modelo:"
   ],
   "metadata": {
    "id": "I6cp1CI3ZKLx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_test_dummies"
   ],
   "metadata": {
    "id": "kQ5AadFSfxhE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "044f7dba-5555-4d62-b84e-494ca9febd99"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "XGB_Regressor = XGBRegressor(min_child_weight = 5, max_depth = 6, learning_rate = 0.3, gamma = 0.1, colsample_bytree = 0.3)\n",
    "XGB_Regressor.fit(df_dummies, df_train_y_regresion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hagamos las predicciones y veamos cómo resultaron las métricas del modelo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediccion_y_metricas_regresion(XGB_Regressor,df_test_dummies, df_test_y_regresion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importancia de features\n",
    "Graficamos los 15 features más importantes para el modelo con los hiperparámetros del TP1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_idx = XGB_Regressor.feature_importances_.argsort()\n",
    "sorted_idx = sorted_idx[-15:]\n",
    "plt.barh(df_test_dummies.columns[sorted_idx], XGB_Regressor.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Xgboost Feature Importance\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos observar que el feature más importante es la ubicación en Puerto Madero, seguido de cantidad de habitaciones y ubicación en Palermo Chico."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Con optimización de hiperparámetros"
   ],
   "metadata": {
    "id": "CkJCoGXnaN1y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cómo se comporta el score con la optimización de hiperparámetros:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Su2wKojB2DpR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_grid = {'learning_rate': [0.20, 0.25, 0.30],\n",
    "               'max_depth': [4, 5, 6, 8, 10],\n",
    "               'min_child_weight': [1, 3, 5, 7, 9],\n",
    "               'gamma': [0.1, 0.2 , 0.3, 0.4, 0.5],\n",
    "               'colsample_bytree' : [0.3, 0.4, 0.5, 0.7, 0.8]}\n",
    "\n",
    "randomCV = RandomizedSearchCV(estimator = XGBRegressor(),\n",
    "                              param_distributions = params_grid,\n",
    "                              scoring = make_scorer(r2_score),\n",
    "                              cv = StratifiedKFold(n_splits = 5),\n",
    "                              n_iter = 5)\n",
    "\n",
    "randomCV.fit(df_dummies, df_train_y_regresion)"
   ],
   "metadata": {
    "id": "7RTpJeq92DpS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c628a9ba-c22a-41d7-fe70-c2cac713f798"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "randomCV.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediccion_y_metricas_regresion(randomCV.best_estimator_,df_test_dummies, df_test_y_regresion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observamos que, si bien las métricas mejoraron, las diferencias al optimizar los parámetros no fueron muy significativas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importancia de features\n",
    "Graficamos los 15 features más importantes para el modelo con los hiperparámetros optimizados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_idx = randomCV.best_estimator_.feature_importances_.argsort()\n",
    "sorted_idx = sorted_idx[-15:]\n",
    "plt.barh(df_test_dummies.columns[sorted_idx], randomCV.best_estimator_.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Xgboost Feature Importance\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos ver que la ubicación en Puerto Madero tiene aún más peso que en el modelo anterior y también como algunos de los features con los que expandimos el dataset (calefaccion_radiante y cochera_fija) toman más protagonismo, al igual que la superficie total."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exportación del Modelo\n",
    "Finalmente exportamos el modelo utilizado para predecir, resultante de la optimización de hiperparámetros:"
   ],
   "metadata": {
    "id": "_f0JxWEL8GUY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/MODELOS/XGB_Regressor.json'\n",
    "else:\n",
    "  path = './MODELOS/XGB_Regressor.json'\n",
    "\n",
    "randomCV.best_estimator_.save_model(path)"
   ],
   "metadata": {
    "id": "RNVI92fe8WEP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e1b9542-5eec-4f40-edbc-ebf7c8cecc42"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Redes Neuronales"
   ],
   "metadata": {
    "collapsed": false,
    "id": "HWpWT3GZ_TQT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.a Regresión"
   ],
   "metadata": {
    "id": "2HjJDe4W_iY-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.1 Preparación del dataset"
   ],
   "metadata": {
    "id": "5xI2H0xyYm_B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "x_train_regresion = df_train_x.drop([\"id\"], axis=1).copy()\n",
    "y_train_regresion = df_train_y_regresion.copy()\n",
    "x_test_regresion = df_test_x.drop([\"id\"], axis=1).copy()\n",
    "y_test_regresion = df_test_y_regresion.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizamos las entradas con StandardScaler:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "PKPRQVFS_TQT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "estandarizar(x_train_regresion, ['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total'])\n",
    "estandarizar(x_test_regresion, ['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total'])"
   ],
   "metadata": {
    "id": "s3xTEOkRlufG"
   },
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.2 Búsqueda del mejor modelo"
   ],
   "metadata": {
    "id": "NjqmQr4pYrEE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creo una función que me permite generar un modelo a partir de sus hiperparámetros. Esta función tiene como parámetros la cantidad de nodos de la primera y anteúltima capa, la cantidad de capas ocultas, la función de activación y el optimizador. Todos los modelos que genera a excepción de los casos sin capas ocultas) tienen forma de 'pirámide'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "cantidad_de_columnas = x_train_regresion.shape[1]\n",
    "\n",
    "def crear_modelo(hidden_layers, first_layer_nodes, last_layer_nodes, activation_func, optimizer):\n",
    "\n",
    "    sequential = Sequential()\n",
    "    sequential.add(keras.layers.Dense(cantidad_de_columnas, input_shape=(cantidad_de_columnas,), activation=activation_func))\n",
    "\n",
    "    if hidden_layers is 0 or hidden_layers is 1:\n",
    "        decremento = 0\n",
    "    else:\n",
    "        decremento = math.ceil((first_layer_nodes - last_layer_nodes) / (hidden_layers - 1))\n",
    "\n",
    "    for i in range (0, hidden_layers):\n",
    "        nodos = first_layer_nodes - decremento * i\n",
    "        sequential.add(Dense(nodos, activation=activation_func))\n",
    "\n",
    "    sequential.add(Dense(1, activation=activation_func))\n",
    "\n",
    "    sequential.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mse', 'mean_absolute_percentage_error']\n",
    "    )\n",
    "\n",
    "    return sequential\n",
    "\n",
    "\n",
    "modelo =  KerasRegressor(build_fn=crear_modelo, verbose = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego busco el mejor modelo a partir de una grilla de parámetros arbitrarios con el método de 'GridSearchCV'. El criterio de mejor modelo es el que tenga menor error cuadrado, o lo que es equivalente, mayor error cuadrado negado. Nos limitamos en la cantidad de folds en el CV por el consumo temporal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    hidden_layers=[0, 1],\n",
    "    first_layer_nodes = [math.ceil(cantidad_de_columnas * 0.7), math.ceil(cantidad_de_columnas * 0.5)],\n",
    "    last_layer_nodes = [5, cantidad_de_columnas * 0.5],\n",
    "    activation_func = ['sigmoid', 'relu', 'tanh'],\n",
    "    batch_size = [100],\n",
    "    epochs = [30],\n",
    "    optimizer=['RMSprop', 'adam'],\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator = modelo,\n",
    "    param_grid = param_grid,\n",
    "    cv=5,\n",
    "    error_score='raise',\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=3,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos todos los modelos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-26 00:19:37.683654: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:37.732506: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:37.740021: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:37.745570: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:39.308206: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:39.320550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:39.407848: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:39.457748: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:39.525168: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32788800 exceeds 10% of free system memory.\n",
      "2022-12-26 00:19:39.544281: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:19:39.579306: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:19:39.646399: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:19:54.349403: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:54.841631: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:54.908857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:55.094087: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:55.796258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:56.098335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:56.186765: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:56.245879: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32788800 exceeds 10% of free system memory.\n",
      "2022-12-26 00:19:56.344263: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:19:56.391609: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:19:56.552448: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:20:11.133218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:20:11.464182: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:20:12.070792: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:20:12.178735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:20:12.521473: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:20:12.688488: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:20:12.810426: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:20:12.971400: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:20:13.556160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:20:13.557259: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 00:20:13.855608: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32788800 exceeds 10% of free system memory.\n",
      "2022-12-26 00:20:13.987930: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:20:28.956637: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:20:29.162479: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n",
      "2022-12-26 00:20:29.281997: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 32789352 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid.fit(x_train_regresion, y_train_regresion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Los parámetros y métricas del mejor modelo fueron:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"El error absoluto porcentual promedio del mejor modelo fue de: \", grid_result.best_estimator_.model.metrics[2].result().numpy())\n",
    "print(\"El error absoluto cuadrado promedio del mejor modelo fue de: \", grid_result.best_estimator_.model.metrics[1].result().numpy())\n",
    "print(\"Los parámetros óptimizados fueron: \", grid.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.3 Predicción"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_predict = grid.best_estimator_.model.evaluate(x_test_regresion, y_test_regresion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.4 Métricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"El error absoluto porcentual promedio fue de: \", grid_predict[2])\n",
    "print(\"El error absoluto cuadrado promedio (mse) fue de: \", grid_predict[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cómo se comporta la función de pérdida del mejor modelo según la cantidad de épocas utilizadas:"
   ],
   "metadata": {
    "id": "vmBg6ev_ZO-e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# epochs = range(60)\n",
    "# historia = grid_result.best_estimator_.model.history.history\n",
    "# historia\n",
    "# plt.plot(epochs, historia['mean_absolute_percentage_error'], color='orange', label='MSE')\n",
    "# plt.xlabel(\"epochs\")\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.title('Error cuadrático medio por cantidad de épocas')\n",
    "# plt.legend()\n",
    "# print(grid_result.best_estimator_.model.metrics[0].result().numpy(), grid_result.best_estimator_.model.metrics[0].name)\n",
    "max(grid_result.cv_results_[\"mean_test_score\"])\n",
    "grid_result.best_estimator_.model.history.params"
   ],
   "metadata": {
    "id": "BzNqiJex_TQU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "outputId": "214e1825-e3d1-4d66-f0db-9b0cdf7a2550",
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = grid.predict(x_test_regresion)\n",
    "\n",
    "mse = metrics.mean_squared_error(\n",
    "    y_true  = y_test_regresion,\n",
    "    y_pred  = y_pred,\n",
    "    squared = True\n",
    ")\n",
    "\n",
    "print(f\"El error según la métrica 'Mean Square Error' de test es: {mse}\")\n",
    "\n",
    "rmse = metrics.mean_squared_error(\n",
    "    y_true  = y_test_regresion,\n",
    "    y_pred  = y_pred,\n",
    "    squared = False\n",
    ")\n",
    "\n",
    "print(f\"El error según la métrica 'Root Mean Square Error' de test es: {rmse}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.5 Exportación de Datos"
   ],
   "metadata": {
    "id": "BeRZVagKeKzO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, exportamos el modelo utilizado:"
   ],
   "metadata": {
    "id": "lO3v9OUzeKzP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/MODELOS/Redes_Regressor.json'\n",
    "else:\n",
    "  path = './MODELOS/Redes_Regressor.json'\n",
    "\n",
    "grid.best_estimator_.save_model(path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ac67727c-79b0-4c22-bb79-7bd9362015f4",
    "id": "tSuqGenveKzP",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.b Clasificación\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "id": "dXdWW-m9_TQV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.b.1 Preparación del dataset"
   ],
   "metadata": {
    "id": "b176QcPLZj9N"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_clasificacion = df_train_x.drop([\"id\"], axis=1).copy()\n",
    "y_train_clasificacion = df_train_y_clasificacion.copy()\n",
    "x_test_clasificacion = df_test_x.drop([\"id\"], axis=1).copy()\n",
    "y_test_clasificacion = df_test_y_clasificacion.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizamos mediante Z-Score:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "JPjRt6tTOWIL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train_clasificacion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']] = zscore(x_train_clasificacion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']], axis=1)\n",
    "\n",
    "x_test_clasificacion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']] = zscore(x_test_clasificacion[['start_date', 'end_date', 'latitud', 'longitud', 'property_rooms', 'property_surface_total']], axis=1)"
   ],
   "metadata": {
    "id": "qDlv11yi8sR1",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aplicamos One Hot encoding a la columna target de entrenamiento y test, para que tenga 3 columnas al igual que la salida del modelo:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "IZ2XeEKi_TQW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y_train_clasificacion = pd.get_dummies(y_train_clasificacion, columns=['tipo_precio_3'], drop_first=False)\n",
    "y_test_clasificacion = pd.get_dummies(y_test_clasificacion, columns=['tipo_precio_3'], drop_first=False)"
   ],
   "metadata": {
    "id": "2jzMgy7b5js1",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.b.2 Búsqueda del mejor modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Función auxiliar para que GridSearchCV realice el scoring de forma correcta."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_categorical_accuracy(y_true, y_pred) :\n",
    "    y_pred_df = pd.DataFrame(data=y_pred, columns=['alto', 'bajo', 'medio'])\n",
    "    res = round(accuracy_score(y_true, y_pred_df), 2)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos una función que permite generar un modelo a partir de sus hiperparámetros configurables.\n",
    "Esta función recibe como parámetros la cantidad de capas ocultas extra, la cantidad de nodos de la última capa oculta, la función de activación y metadata del clasificador."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def crear_modelo(extra_hidden_layers, last_layer_nodes, activation_func, optimizer, metricas, loss_func, meta):\n",
    "\n",
    "    n_features_in_ = meta[\"n_features_in_\"]\n",
    "    X_shape_ = meta[\"X_shape_\"]\n",
    "    n_classes_ = meta[\"n_classes_\"]\n",
    "\n",
    "    sequential = Sequential()\n",
    "    sequential.add(keras.layers.Dense(n_features_in_ * 0.7, input_shape=X_shape_[1:], activation=activation_func))\n",
    "\n",
    "    if last_layer_nodes > n_features_in_:\n",
    "        decremento = 0\n",
    "    elif extra_hidden_layers is 0 or extra_hidden_layers is 1:\n",
    "        decremento = 0\n",
    "    else:\n",
    "        decremento = math.ceil((n_features_in_ - last_layer_nodes) / (extra_hidden_layers - 1))\n",
    "\n",
    "    for i in range (0, extra_hidden_layers):\n",
    "        nodos = n_features_in_ - decremento * i\n",
    "        sequential.add(Dense(nodos, activation=activation_func))\n",
    "\n",
    "    sequential.add(Dense(n_classes_, activation='softmax'))\n",
    "\n",
    "    return sequential\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos un modelo base, necesario según la documentación de SciKeras - KerasClassifier.\n",
    "Este modelo base será editado por GridSearchCV al buscar los hiperparámetros."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model = KerasClassifier(crear_modelo,\n",
    "                             loss=\"categorical_crossentropy\",\n",
    "                             extra_hidden_layers=1,\n",
    "                             last_layer_nodes=1,\n",
    "                             activation_func='sigmoid',\n",
    "                             )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos los hiperparámetros posibles. Además de los necesarios por la función crear_modelo, agregamos diferentes optimizadores, learning rates y cantidad de épocas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    extra_hidden_layers=[0, 1],\n",
    "    last_layer_nodes = [10, 20],\n",
    "    activation_func = ['relu', 'softmax'],\n",
    "    batch_size = [100],\n",
    "    epochs = [5, 20],\n",
    "    optimizer__learning_rate = [0.001, 0.0001],\n",
    "    optimizer = [\"adam\", \"sge\"],\n",
    "    loss_func = [\"categorical_crossentropy\"],\n",
    ")\n",
    "\n",
    "gs = GridSearchCV(base_model,\n",
    "                  param_grid = param_grid,\n",
    "                  cv=5,\n",
    "                  scoring=make_scorer(my_categorical_accuracy),\n",
    "                  verbose=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos los diferentes modelos y obtenemos los mejores parámetros."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gs.fit(x_train_clasificacion, y_train_clasificacion)\n",
    "print(\"Los parámetros óptimizados fueron: \", gs.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predecimos con el dataset de test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pr = gs.predict(x_test_clasificacion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.b.4 Métricas\n",
    "Transformamos el array devuelto por el modelo a un dataframe de iguales columnas que y_test_clasificacion"
   ],
   "metadata": {
    "id": "W-KMjjHwaFjg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pr_df = pd.DataFrame(data=y_pr, columns=['alto', 'bajo', 'medio'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculamos las métricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metricas_clasificacion(y_test_clasificacion, y_pr_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.a.5 Exportación de Datos"
   ],
   "metadata": {
    "id": "5SsWSPw1fJYm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, exportamos el modelo utilizado:"
   ],
   "metadata": {
    "id": "IlgF2MmmfJYn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/MODELOS/Redes_Classifier.json'\n",
    "else:\n",
    "  path = './MODELOS/Redes_Classifier.json'\n",
    "\n",
    "gs.best_estimator_.save_model(path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "48479ec0-085c-4964-aa1d-b771dc5643e4",
    "id": "yjKFj8TjfJYn",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDpREdwKxB6m"
   },
   "source": [
    "## 3. Ensamble de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvLq10beDadg"
   },
   "source": [
    "### 3.1 Ensamble Híbrido: Voting - Clasificación\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAT6HU3buhLU"
   },
   "source": [
    "#### 3.1.1 Preparación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZNjL2W-vZvC"
   },
   "source": [
    "Para la parte de ensambles, lo que haremos será utilizar nuevamente el dataset al cual se le aplicó una reducción de su dimensionalidad en el trabajo práctico n°1. \n",
    "\n",
    "Para esto lo que haremos será trabajar con una copia del dataset modificado al inicio del trabajo, el cual usa como base el reducido mencionado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDp1eW_cHfxt"
   },
   "source": [
    "Para nuestra variable `target`, utilizaremos como convención la misma que fue planteada para el tp1. Ésta consiste en subdividir a la variable pxm2 (precio por metro cuadrado) en 3 intervalos, 25% a bajo, 50% a medio y el otro 25% restante a alto. A su vez, se hará la separación tambien por tipo de propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T0ZW9l-dcqI",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_train_voting = x_train_clasificacion\n",
    "y_train_voting = y_train_clasificacion\n",
    "\n",
    "x_test_voting = x_test_clasificacion\n",
    "y_test_voting = y_test_clasificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "A modo de finalización de este trabajo, haremos un pequeño repaso sobre todos los puntos que analizamos y sus respectivos resultados y/o observaciones hechos por nosotros:\n",
    "\n",
    "Sobre el análisis realizado en el apartado de **procesamiento del lenguaje natural**, y en particular haciendo referencia a las métricas que obtuvimos luego del entrenamiento y predicción con el dataset producto de la ampliación, utilizando el modelo XGBoost, pudimos observar que si bien aún las métricas no nos resultan del todo satisfactorias cumplen con el objetivo de mejorar las resultantes del trabajo anterior. Más allá de que la cantidad de aciertos continúa sin incrementar, tanto el error MSE como el RMSE decrecieron ofreciendo mejores resultados. Por otro lado, el coeficiente de determinación se incrementó notoriamente llegando casi a un 0.9%. \n",
    "\n",
    "En cuanto a lo referido sobre **redes neuronales**, la comparación será directamente en base a los nuevos modelos ya que el dataset utilizado es compartido en ambos trabajos. Dicho esto, el error cometido en el modelo de regresión es significativamente pequeño. Teniendo en cuenta los resultantes del TP1, si utilizamos el menor luego de que éstos tuvieron su optimización de parámetros correspondiente, notamos que el que mejor performa es XGBoost con un MSE de 51650994876 (al cual de todas formas le atribuímos arrastre de error en nuestro planteo). Utilizando redes neuronales cometemos tan solo un error cuadrático medio equivalente a 1.36. Por otro lado, en lo que respecta al modelo de clasificación, obtuvimos resultados bastante similares. Si bien tanto el accuracy, precisión y F1-Score dieron distintos, la diferencia es de tan solo uno/dos puntos. Con esto concluimos que para clasificación no se notan mejoras por sobre los modelos utilizados en el trabajo anterior.\n",
    "\n",
    "Como último requerimiento se pidió estudiar las métricas tanto para regresión como clasificación pero haciendo uso de **ensambles**, en particular de tipo híbridos. En lo que respecta al utilizado para clasificación (Voting) obtuvimos mejores resultados para las métricas de Accuracy, Recall y F1-Score. Sin embargo, aunque en Precisión disminuyó un poco, esta diferencia continúa siendo no significativa. Pasando al ensamble de regresión (Stacking) el MSE resultante que obtuvimos fue 238481755830. Creemos que nuevamente estamos cometiendo involuntariamente algún arrastre de error en alguno de los incisos desarrollados, ya que este error es incluso mayor a los cometidos en los modelos individuales desarrollados en el TP1, y no estaría respetando la regla principal de ‘*La sabiduría de las multitudes*’."
   ],
   "metadata": {
    "id": "CE40zH4PI4xY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bajo = y_train_voting['bajo'].sum() + y_test_voting[y_test_voting == 'bajo'].count()\n",
    "medio = y_train_voting['medio'].sum() + y_test_voting[y_test_voting == 'medio'].count()\n",
    "alto = y_train_voting['alto'].sum() + y_test_voting[y_test_voting == 'alto'].count()\n",
    "\n",
    "print(f\"Se observaron: \\n - {round(bajo,3)} registros de tipo 'bajo'. \\n - {round(medio,3)} registros de tipo 'medio'. \\n - {round(alto,3)} registros de tipo 'alto'.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.2 Definición del Ensamble"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para el tipo de ensamble **voting**, lo que necesitaremos será contar con `n` cantidad de modelos previamente entrenados para luego someterlos a una votación. De la misma, saldrá la clasificación para la nueva instancia en base a lo que indique la mayoría de ellos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Elegimos tomar como modelos los mismos empleados en el TP1:\n",
    "\n",
    "\n",
    "*   Árbol de Decisión\n",
    "*   Random Forest\n",
    "*   KNN\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcs_clf = DecisionTreeClassifier()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "knn_clf = KNeighborsClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez que contamos con los modelos que vamos a utilizar en el ensamble, procedemos a su creación. En este caso particular decidimos utilizar el tipo de votación hard el cual utilizará la regla de la mayoría. Por otro lado, utilizamos el hiperparámetro `estimators` para definir como nos vamos a referir a dichos modelos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vot_clf = VotingClassifier(estimators = [('dcs', dcs_clf), ('rnd', rnd_clf), ('knn', knn_clf)], voting = 'hard')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.3 Entrenamiento y Predicción"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reorganizamos los valores predichos para que queden en una sola columna, y comparamos con el conjunto de prueba:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train_voting_arr = np.apply_along_axis(convert_b_m_a, axis=1, arr=y_train_voting)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seguimos reacomodando los valores para poder calcular las métricas correspondientes, notemos que no pueden guardarse valores equivalentes al string 'medio' en el array:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(y_train_voting_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aplicamos estrategias de transformación de datos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train_voting_arr = np.where(y_train_voting_arr == 'medi', 'medio', y_train_voting_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(y_train_voting_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, entrenamos el ensamle:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vot_clf.fit(x_train_voting, y_train_voting_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_voting = vot_clf.predict(x_test_voting)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.4 Métricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para poder determinar que tan bueno resulto el modelo, lo que haremos será observar las `métricas` resultantes de una predicción con los datos de test. Recordemos que nuestras funcinoes de metricas fueron definidas al inicio de este trabajo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metricas_clasificacion(y_test_voting, y_pred_voting)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A su vez, también podemos visualizar los mismos a través de la siguiente matriz de confusión."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test_voting, y_pred_voting)\n",
    "sns.heatmap(matrix,cmap='GnBu',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.4 Exportación de Datos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, exportamos el modelo utilizado:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/MODELOS/Voting_Classifier.joblib'\n",
    "else:\n",
    "  path = './MODELOS/Voting_Classifier.joblib'\n",
    "\n",
    "dump(vot_clf, path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Ensamble Híbrido: Stacking - Regresión\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.1 Preparación del dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nuevamente, utilizamos los datasets de train y test previamente separados:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_stacking = x_train_regresion\n",
    "y_train_stacking = y_train_regresion\n",
    "\n",
    "x_test_stacking = x_test_regresion\n",
    "y_test_stacking = y_test_regresion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.2 Definición del Ensamble\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lo que haremos en esta nueva sección, será implementar un nuevo tipo de ensable híbrido con la salvedad de que esta vez utilizaremos el tipo `cascading`.\n",
    "El mismo se basa en el entrenamiento de distintos `modelos base`, y a su vez utilizará un `meta-modelo` el cual realizará su predicción en base a las predicciones de los diferentes modelos comentados anteriormente.\n",
    "\n",
    "Como es indicado por el enunciado del trabajo se utilizarán modelos de regresión. En particular, decidimos trabajar con:\n",
    "\n",
    "\n",
    "*   KNN\n",
    "*   XGBoost\n",
    "*   AdaBoost\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_rgs = KNeighborsRegressor()\n",
    "xgb_rgs = XGBRegressor()\n",
    "adb_rgs = AdaBoostRegressor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego utilizamos los mismos para definir nuestro modelo base, en el cual luego se basará el meta-modelo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_models = [('KNN', knn_rgs),\n",
    "               ('XGBoost', xgb_rgs),\n",
    "               ('AdaBoost', adb_rgs)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como mencionamos, a continuación los utilizaremos para definir nuestro meta-modelo. Para este último, decidimos emplear el modelo de regresión logistica."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "meta_model = GradientBoostingRegressor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos nuestro ensamble indicando como modelos estimadores Knn, XGBoost y AdaBoost, y como estimador final el modelo de Regresión Lineal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s = 5\n",
    "n = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stacking_model = StackingRegressor(estimators=base_models,\n",
    "                                    final_estimator=meta_model,\n",
    "                                    passthrough=True,\n",
    "                                    cv=s,\n",
    "                                    verbose=n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.3 Entrenamiento y Predicción"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_scores = defaultdict()\n",
    "\n",
    "for name, model in base_models:\n",
    "    print('Evaluating {}'.format(name))\n",
    "    scores = evaluate_model(model, x_train_stacking, y_train_stacking, s, n)\n",
    "    model_scores[name] = scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos cómo resultaron los scores de los modelos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(model_scores, name='stacking_model_cv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos el modelo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stacking_model.fit(x_train_stacking, y_train_stacking)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente haremos la predicción:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_stacking = stacking_model.predict(x_test_stacking)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.4 Métricas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para poder evaluar la performance que obtuvo nuestro modelo utilizaremos la métrica de evaluación del error cuadrático medio (MSE) como se pide por enunciado."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mse = metrics.mean_squared_error(\n",
    "        y_true  = y_test_stacking,\n",
    "        y_pred  = y_pred_stacking,\n",
    "        squared = True\n",
    "       )\n",
    "\n",
    "print(f\"El error según la métrica 'Mean Square Error' de test es: {mse}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.4 Exportación de Datos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, exportamos el modelo utilizado:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  path = '/content/drive/MyDrive/📔 Organización de Datos (75.06)/TPS/TP2/MODELOS/Stacking_Regressor.joblib'\n",
    "else:\n",
    "  path = './MODELOS/Stacking_Regressor.joblib'\n",
    "\n",
    "dump(stacking_model, path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusiones"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "EAT6HU3buhLU",
    "_kG7ZRlKuaxT",
    "D8sgMCouvLx-"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
